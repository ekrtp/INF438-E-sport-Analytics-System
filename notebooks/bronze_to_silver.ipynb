{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3026294",
   "metadata": {},
   "outputs": [],
   "source": "# Notebook: Bronze to Silver Transformation\n# Purpose: Clean and transform Bronze data to Silver layer\n\nfrom pyspark.sql.functions import *\nimport os\n\nstorage_account = \"dota2lakehousenew\"\ncontainer = \"data\"\nstorage_account_key = os.environ.get(\"AZURE_STORAGE_KEY\")\n\nspark.conf.set(\n    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n    storage_account_key\n)\n\nBRONZE_PATH = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze\"\nSILVER_PATH = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver\"\n\nprint(\"Bronze to Silver data processing started\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23754e3a",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nLoading data from Bronze layer...\")\n\ndf_matches_raw = spark.read.csv(\n    f\"{BRONZE_PATH}/main_metadata.csv\",\n    header=True,\n    inferSchema=True\n)\n\ndf_players_raw = spark.read.csv(\n    f\"{BRONZE_PATH}/players_reduced.csv\",\n    header=True,\n    inferSchema=True\n)\n\ndf_picks_bans_raw = spark.read.csv(\n    f\"{BRONZE_PATH}/picks_bans.csv\",\n    header=True,\n    inferSchema=True\n)\n\ndf_teams_raw = spark.read.csv(\n    f\"{BRONZE_PATH}/teams.csv\",\n    header=True,\n    inferSchema=True\n)\n\nprint(f\"Matches: {df_matches_raw.count():,} rows, {len(df_matches_raw.columns)} columns\")\nprint(f\"Players: {df_players_raw.count():,} rows, {len(df_players_raw.columns)} columns\")\nprint(f\"Picks/Bans: {df_picks_bans_raw.count():,} rows, {len(df_picks_bans_raw.columns)} columns\")\nprint(f\"Teams: {df_teams_raw.count():,} rows, {len(df_teams_raw.columns)} columns\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e5df01",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nData quality assessment...\")\n\ndef analyze_nulls(df, name):\n    print(f\"\\n{name}:\")\n    total_rows = df.count()\n    null_cols = []\n    for col_name in df.columns:\n        null_count = df.filter(col(col_name).isNull()).count()\n        if null_count > 0:\n            pct = (null_count / total_rows) * 100\n            null_cols.append((col_name, null_count, pct))\n    \n    if null_cols:\n        print(f\"  Columns with nulls (top 10):\")\n        for c, cnt, pct in sorted(null_cols, key=lambda x: -x[1])[:10]:\n            print(f\"    {c}: {cnt:,} ({pct:.1f}%)\")\n    else:\n        print(f\"  No null values found\")\n    return null_cols\n\nmatches_nulls = analyze_nulls(df_matches_raw, \"Matches\")\nplayers_nulls = analyze_nulls(df_players_raw, \"Players\")\n\ndup_matches = df_matches_raw.count() - df_matches_raw.dropDuplicates([\"match_id\"]).count()\ndup_players = df_players_raw.count() - df_players_raw.dropDuplicates([\"match_id\", \"player_slot\"]).count()\n\nprint(f\"\\nDuplicate matches: {dup_matches}\")\nprint(f\"Duplicate player records: {dup_players}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d492fc",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nCleaning Matches data...\")\n\ndf_matches_clean = df_matches_raw \\\n    .drop(\"Unnamed: 0\") \\\n    .dropDuplicates([\"match_id\"]) \\\n    .filter(col(\"match_id\").isNotNull()) \\\n    .filter(col(\"duration\").isNotNull()) \\\n    .filter(col(\"duration\") > 300) \\\n    .filter(col(\"duration\") < 7200)\n\ndf_matches_clean = df_matches_clean \\\n    .withColumn(\"duration_minutes\", round(col(\"duration\") / 60, 2)) \\\n    .withColumn(\"radiant_win\", col(\"radiant_win\").cast(\"boolean\")) \\\n    .withColumn(\"match_date\", to_date(col(\"start_date_time\"))) \\\n    .withColumn(\"match_hour\", hour(col(\"start_date_time\"))) \\\n    .withColumn(\"match_day_of_week\", dayofweek(col(\"start_date_time\"))) \\\n    .withColumn(\"total_kills\", col(\"radiant_score\") + col(\"dire_score\")) \\\n    .withColumn(\"kill_difference\", abs(col(\"radiant_score\") - col(\"dire_score\"))) \\\n    .withColumn(\"is_stomp\",\n        when(col(\"stomp\") == 1.0, True)\n        .when(col(\"kill_difference\") > 30, True)\n        .otherwise(False)\n    ) \\\n    .withColumn(\"ingestion_timestamp\", current_timestamp())\n\nmatches_silver_cols = [\n    \"match_id\", \"duration\", \"duration_minutes\", \"radiant_win\",\n    \"radiant_score\", \"dire_score\", \"total_kills\", \"kill_difference\",\n    \"first_blood_time\", \"game_mode\", \"lobby_type\", \"leagueid\",\n    \"match_date\", \"match_hour\", \"match_day_of_week\",\n    \"radiant_team_id\", \"dire_team_id\", \"region\", \"patch\",\n    \"is_stomp\", \"throw\", \"comeback\", \"ingestion_timestamp\"\n]\n\nexisting_cols = [c for c in matches_silver_cols if c in df_matches_clean.columns]\ndf_matches_silver = df_matches_clean.select(existing_cols)\n\nprint(f\"Matches before: {df_matches_raw.count():,}\")\nprint(f\"Matches after: {df_matches_silver.count():,}\")\nprint(f\"Removed: {df_matches_raw.count() - df_matches_silver.count():,} rows\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea7770",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nCleaning Players data...\")\n\nplayer_cols_to_keep = [\n    \"match_id\", \"player_slot\", \"account_id\", \"hero_id\",\n    \"kills\", \"deaths\", \"assists\", \"kda\",\n    \"gold_per_min\", \"xp_per_min\", \"net_worth\", \"total_gold\", \"total_xp\",\n    \"hero_damage\", \"hero_healing\", \"tower_damage\",\n    \"last_hits\", \"denies\", \"level\",\n    \"isRadiant\", \"win\", \"lose\",\n    \"lane\", \"lane_role\", \"is_roaming\",\n    \"camps_stacked\", \"stuns\", \"teamfight_participation\",\n    \"observer_kills\", \"sentry_kills\",\n    \"personaname\", \"rank_tier\"\n]\n\nexisting_player_cols = [c for c in player_cols_to_keep if c in df_players_raw.columns]\n\ndf_players_clean = df_players_raw \\\n    .drop(\"Unnamed: 0\") \\\n    .select(existing_player_cols) \\\n    .dropDuplicates([\"match_id\", \"player_slot\"]) \\\n    .filter(col(\"match_id\").isNotNull())\n\ndf_players_clean = df_players_clean \\\n    .withColumn(\"kills\", col(\"kills\").cast(\"int\")) \\\n    .withColumn(\"deaths\", col(\"deaths\").cast(\"int\")) \\\n    .withColumn(\"assists\", col(\"assists\").cast(\"int\")) \\\n    .withColumn(\"kda\", col(\"kda\").cast(\"double\")) \\\n    .withColumn(\"gold_per_min\", col(\"gold_per_min\").cast(\"double\")) \\\n    .withColumn(\"xp_per_min\", col(\"xp_per_min\").cast(\"double\")) \\\n    .withColumn(\"hero_damage\", col(\"hero_damage\").cast(\"long\")) \\\n    .withColumn(\"tower_damage\", col(\"tower_damage\").cast(\"long\")) \\\n    .withColumn(\"win\", col(\"win\").cast(\"int\")) \\\n    .fillna({\"kills\": 0, \"deaths\": 0, \"assists\": 0, \"kda\": 0})\n\ndf_players_clean = df_players_clean \\\n    .withColumn(\"kda_calculated\",\n        when(col(\"deaths\") == 0, col(\"kills\") + col(\"assists\"))\n        .otherwise(round((col(\"kills\") + col(\"assists\")) / col(\"deaths\"), 2))\n    )\n\ndf_players_silver = df_players_clean \\\n    .withColumn(\"ingestion_timestamp\", current_timestamp())\n\nprint(f\"Players before: {df_players_raw.count():,}\")\nprint(f\"Players after: {df_players_silver.count():,}\")\nprint(f\"Columns: {len(df_players_raw.columns)} -> {len(df_players_silver.columns)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2721b",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nCleaning Picks/Bans data...\")\n\ndf_picks_bans_clean = df_picks_bans_raw \\\n    .drop(\"Unnamed: 0\") \\\n    .filter(col(\"match_id\").isNotNull()) \\\n    .filter(col(\"hero_id\").isNotNull()) \\\n    .withColumn(\"is_pick\", col(\"is_pick\").cast(\"boolean\")) \\\n    .withColumn(\"hero_id\", col(\"hero_id\").cast(\"int\")) \\\n    .withColumn(\"team\", col(\"team\").cast(\"int\")) \\\n    .withColumn(\"pick_order\", col(\"order\").cast(\"int\")) \\\n    .drop(\"order\", \"ord\") \\\n    .withColumn(\"ingestion_timestamp\", current_timestamp())\n\nprint(f\"Picks/Bans cleaned: {df_picks_bans_clean.count():,} rows\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcb8f4a",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nOutlier detection...\")\n\nstats = df_players_silver.select(\n    mean(\"kills\").alias(\"mean_kills\"),\n    stddev(\"kills\").alias(\"std_kills\"),\n    mean(\"deaths\").alias(\"mean_deaths\"),\n    stddev(\"deaths\").alias(\"std_deaths\"),\n    mean(\"gold_per_min\").alias(\"mean_gpm\"),\n    stddev(\"gold_per_min\").alias(\"std_gpm\")\n).collect()[0]\n\nprint(f\"Player Statistics:\")\nprint(f\"  Kills: mean={stats['mean_kills']:.2f}, std={stats['std_kills']:.2f}\")\nprint(f\"  Deaths: mean={stats['mean_deaths']:.2f}, std={stats['std_deaths']:.2f}\")\nprint(f\"  GPM: mean={stats['mean_gpm']:.2f}, std={stats['std_gpm']:.2f}\")\n\ndf_players_silver = df_players_silver.withColumn(\n    \"is_outlier\",\n    when(\n        (col(\"kills\") > stats[\"mean_kills\"] + 3 * stats[\"std_kills\"]) |\n        (col(\"deaths\") > stats[\"mean_deaths\"] + 3 * stats[\"std_deaths\"]) |\n        (col(\"gold_per_min\") > stats[\"mean_gpm\"] + 3 * stats[\"std_gpm\"]),\n        True\n    ).otherwise(False)\n)\n\noutlier_count = df_players_silver.filter(col(\"is_outlier\") == True).count()\nprint(f\"Outliers detected: {outlier_count:,} ({outlier_count/df_players_silver.count()*100:.2f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c5bcc",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nWriting to Silver Layer...\")\n\ndf_matches_silver.write \\\n    .format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .save(f\"{SILVER_PATH}/cleaned_matches\")\nprint(f\"Matches written to: {SILVER_PATH}/cleaned_matches\")\n\ndf_players_silver.write \\\n    .format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .save(f\"{SILVER_PATH}/cleaned_players\")\nprint(f\"Players written to: {SILVER_PATH}/cleaned_players\")\n\ndf_picks_bans_clean.write \\\n    .format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .save(f\"{SILVER_PATH}/cleaned_picks_bans\")\nprint(f\"Picks/Bans written to: {SILVER_PATH}/cleaned_picks_bans\")\n\nprint(\"\\nSilver layer complete\")"
  },
  {
   "cell_type": "code",
   "id": "ckynwakf8rv",
   "source": "print(\"\\nExporting CSV samples...\")\n\nfrom pyspark.sql.functions import rand\nfrom datetime import datetime\n\nSAMPLE_SIZE_SILVER = 1000\nSAMPLES_PATH = f\"{SILVER_PATH}/samples\"\ntimestamp = datetime.now().strftime(\"%Y%m%d\")\n\nprint(f\"Exporting {SAMPLE_SIZE_SILVER} matches...\")\nmatches_sample = df_matches_silver.orderBy(rand(seed=42)).limit(SAMPLE_SIZE_SILVER)\nmatches_sample_file = f\"{SAMPLES_PATH}/silver_cleaned_matches_sample_{SAMPLE_SIZE_SILVER}rows_{timestamp}.csv\"\nmatches_sample.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(matches_sample_file)\nprint(f\"  Written to: {matches_sample_file}\")\n\nprint(f\"Exporting {SAMPLE_SIZE_SILVER} player records...\")\nplayers_sample = df_players_silver.orderBy(rand(seed=42)).limit(SAMPLE_SIZE_SILVER)\nplayers_sample_file = f\"{SAMPLES_PATH}/silver_cleaned_players_sample_{SAMPLE_SIZE_SILVER}rows_{timestamp}.csv\"\nplayers_sample.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(players_sample_file)\nprint(f\"  Written to: {players_sample_file}\")\n\nprint(f\"Exporting {SAMPLE_SIZE_SILVER} picks/bans records...\")\npicks_bans_sample = df_picks_bans_clean.orderBy(rand(seed=42)).limit(SAMPLE_SIZE_SILVER)\npicks_bans_sample_file = f\"{SAMPLES_PATH}/silver_cleaned_picks_bans_sample_{SAMPLE_SIZE_SILVER}rows_{timestamp}.csv\"\npicks_bans_sample.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(picks_bans_sample_file)\nprint(f\"  Written to: {picks_bans_sample_file}\")\n\nprint(f\"\\nSilver samples exported successfully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b5982f",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nVerification: Reading back from Silver...\")\n\ndf_verify_matches = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/cleaned_matches\")\ndf_verify_players = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/cleaned_players\")\n\nprint(f\"Silver Matches: {df_verify_matches.count():,} rows\")\nprint(f\"Silver Players: {df_verify_players.count():,} rows\")\n\nprint(\"\\nSample Matches:\")\ndf_verify_matches.select(\"match_id\", \"duration_minutes\", \"radiant_win\", \"total_kills\", \"match_date\").show(5)\n\nprint(\"\\nSample Players:\")\ndf_verify_players.select(\"match_id\", \"account_id\", \"hero_id\", \"kills\", \"deaths\", \"assists\", \"kda\", \"win\").show(5)\n\nprint(\"\\nCSV Samples created:\")\ntry:\n    sample_files = dbutils.fs.ls(f\"{SILVER_PATH}/samples\")\n    for f in sample_files:\n        if f.name.endswith('.csv/'):\n            print(f\"  {f.name}\")\nexcept Exception as e:\n    print(f\"  (No samples directory yet)\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}