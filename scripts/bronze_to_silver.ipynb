{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3026294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: 01_bronze_to_silver\n",
    "# Purpose: Clean and transform Bronze data to Silver layer\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "storage_account = \"dota2lakehousenew\"\n",
    "container = \"data\"\n",
    "storage_account_key = os.environ.get(\"AZURE_STORAGE_KEY\")\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    storage_account_key\n",
    ")\n",
    "\n",
    "BRONZE_PATH = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze\"\n",
    "SILVER_PATH = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BRONZE TO SILVER DATA PROCESSING\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23754e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LOAD RAW DATA FROM BRONZE\n",
    "print(\"\\n[1/6] Loading data from Bronze layer...\")\n",
    "\n",
    "df_matches_raw = spark.read.csv(\n",
    "    f\"{BRONZE_PATH}/main_metadata.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_players_raw = spark.read.csv(\n",
    "    f\"{BRONZE_PATH}/players_reduced.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_picks_bans_raw = spark.read.csv(\n",
    "    f\"{BRONZE_PATH}/picks_bans.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_teams_raw = spark.read.csv(\n",
    "    f\"{BRONZE_PATH}/teams.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"   Matches loaded: {df_matches_raw.count():,} rows, {len(df_matches_raw.columns)} columns\")\n",
    "print(f\"   Players loaded: {df_players_raw.count():,} rows, {len(df_players_raw.columns)} columns\")\n",
    "print(f\"   Picks/Bans loaded: {df_picks_bans_raw.count():,} rows, {len(df_picks_bans_raw.columns)} columns\")\n",
    "print(f\"   Teams loaded: {df_teams_raw.count():,} rows, {len(df_teams_raw.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e5df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DATA QUALITY ASSESSMENT\n",
    "print(\"\\n[2/6] Data Quality Assessment...\")\n",
    "\n",
    "def analyze_nulls(df, name):\n",
    "    print(f\"\\n  --- {name} ---\")\n",
    "    total_rows = df.count()\n",
    "    null_cols = []\n",
    "    for col_name in df.columns:\n",
    "        null_count = df.filter(col(col_name).isNull()).count()\n",
    "        if null_count > 0:\n",
    "            pct = (null_count / total_rows) * 100\n",
    "            null_cols.append((col_name, null_count, pct))\n",
    "    \n",
    "    if null_cols:\n",
    "        print(f\"  Columns with nulls (top 10):\")\n",
    "        for c, cnt, pct in sorted(null_cols, key=lambda x: -x[1])[:10]:\n",
    "            print(f\"    {c}: {cnt:,} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"  No null values found!\")\n",
    "    return null_cols\n",
    "\n",
    "matches_nulls = analyze_nulls(df_matches_raw, \"Matches\")\n",
    "players_nulls = analyze_nulls(df_players_raw, \"Players\")\n",
    "\n",
    "dup_matches = df_matches_raw.count() - df_matches_raw.dropDuplicates([\"match_id\"]).count()\n",
    "dup_players = df_players_raw.count() - df_players_raw.dropDuplicates([\"match_id\", \"player_slot\"]).count()\n",
    "\n",
    "print(f\"\\n  Duplicate matches: {dup_matches}\")\n",
    "print(f\"  Duplicate player records: {dup_players}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d492fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CLEAN MATCHES DATA\n",
    "print(\"\\n[3/6] Cleaning Matches data...\")\n",
    "\n",
    "df_matches_clean = df_matches_raw \\\n",
    "    .drop(\"Unnamed: 0\") \\\n",
    "    .dropDuplicates([\"match_id\"]) \\\n",
    "    .filter(col(\"match_id\").isNotNull()) \\\n",
    "    .filter(col(\"duration\").isNotNull()) \\\n",
    "    .filter(col(\"duration\") > 300) \\\n",
    "    .filter(col(\"duration\") < 7200)\n",
    "\n",
    "df_matches_clean = df_matches_clean \\\n",
    "    .withColumn(\"duration_minutes\", round(col(\"duration\") / 60, 2)) \\\n",
    "    .withColumn(\"radiant_win\", col(\"radiant_win\").cast(\"boolean\")) \\\n",
    "    .withColumn(\"match_date\", to_date(col(\"start_date_time\"))) \\\n",
    "    .withColumn(\"match_hour\", hour(col(\"start_date_time\"))) \\\n",
    "    .withColumn(\"match_day_of_week\", dayofweek(col(\"start_date_time\"))) \\\n",
    "    .withColumn(\"total_kills\", col(\"radiant_score\") + col(\"dire_score\")) \\\n",
    "    .withColumn(\"kill_difference\", abs(col(\"radiant_score\") - col(\"dire_score\"))) \\\n",
    "    .withColumn(\"is_stomp\",\n",
    "        when(col(\"stomp\") == 1.0, True)\n",
    "        .when(col(\"kill_difference\") > 30, True)\n",
    "        .otherwise(False)\n",
    "    ) \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "matches_silver_cols = [\n",
    "    \"match_id\", \"duration\", \"duration_minutes\", \"radiant_win\",\n",
    "    \"radiant_score\", \"dire_score\", \"total_kills\", \"kill_difference\",\n",
    "    \"first_blood_time\", \"game_mode\", \"lobby_type\", \"leagueid\",\n",
    "    \"match_date\", \"match_hour\", \"match_day_of_week\",\n",
    "    \"radiant_team_id\", \"dire_team_id\", \"region\", \"patch\",\n",
    "    \"is_stomp\", \"throw\", \"comeback\", \"ingestion_timestamp\"\n",
    "]\n",
    "\n",
    "existing_cols = [c for c in matches_silver_cols if c in df_matches_clean.columns]\n",
    "df_matches_silver = df_matches_clean.select(existing_cols)\n",
    "\n",
    "print(f\"   Matches before cleaning: {df_matches_raw.count():,}\")\n",
    "print(f\"   Matches after cleaning: {df_matches_silver.count():,}\")\n",
    "print(f\"   Removed: {df_matches_raw.count() - df_matches_silver.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea7770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. CLEAN PLAYERS DATA\n",
    "print(\"\\n[4/6] Cleaning Players data...\")\n",
    "\n",
    "player_cols_to_keep = [\n",
    "    \"match_id\", \"player_slot\", \"account_id\", \"hero_id\",\n",
    "    \"kills\", \"deaths\", \"assists\", \"kda\",\n",
    "    \"gold_per_min\", \"xp_per_min\", \"net_worth\", \"total_gold\", \"total_xp\",\n",
    "    \"hero_damage\", \"hero_healing\", \"tower_damage\",\n",
    "    \"last_hits\", \"denies\", \"level\",\n",
    "    \"isRadiant\", \"win\", \"lose\",\n",
    "    \"lane\", \"lane_role\", \"is_roaming\",\n",
    "    \"camps_stacked\", \"stuns\", \"teamfight_participation\",\n",
    "    \"observer_kills\", \"sentry_kills\",\n",
    "    \"personaname\", \"rank_tier\"\n",
    "]\n",
    "\n",
    "existing_player_cols = [c for c in player_cols_to_keep if c in df_players_raw.columns]\n",
    "\n",
    "df_players_clean = df_players_raw \\\n",
    "    .drop(\"Unnamed: 0\") \\\n",
    "    .select(existing_player_cols) \\\n",
    "    .dropDuplicates([\"match_id\", \"player_slot\"]) \\\n",
    "    .filter(col(\"match_id\").isNotNull())\n",
    "\n",
    "df_players_clean = df_players_clean \\\n",
    "    .withColumn(\"kills\", col(\"kills\").cast(\"int\")) \\\n",
    "    .withColumn(\"deaths\", col(\"deaths\").cast(\"int\")) \\\n",
    "    .withColumn(\"assists\", col(\"assists\").cast(\"int\")) \\\n",
    "    .withColumn(\"kda\", col(\"kda\").cast(\"double\")) \\\n",
    "    .withColumn(\"gold_per_min\", col(\"gold_per_min\").cast(\"double\")) \\\n",
    "    .withColumn(\"xp_per_min\", col(\"xp_per_min\").cast(\"double\")) \\\n",
    "    .withColumn(\"hero_damage\", col(\"hero_damage\").cast(\"long\")) \\\n",
    "    .withColumn(\"tower_damage\", col(\"tower_damage\").cast(\"long\")) \\\n",
    "    .withColumn(\"win\", col(\"win\").cast(\"int\")) \\\n",
    "    .fillna({\"kills\": 0, \"deaths\": 0, \"assists\": 0, \"kda\": 0})\n",
    "\n",
    "df_players_clean = df_players_clean \\\n",
    "    .withColumn(\"kda_calculated\",\n",
    "        when(col(\"deaths\") == 0, col(\"kills\") + col(\"assists\"))\n",
    "        .otherwise(round((col(\"kills\") + col(\"assists\")) / col(\"deaths\"), 2))\n",
    "    )\n",
    "\n",
    "df_players_silver = df_players_clean \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "print(f\"   Players before cleaning: {df_players_raw.count():,}\")\n",
    "print(f\"   Players after cleaning: {df_players_silver.count():,}\")\n",
    "print(f\"   Columns reduced: {len(df_players_raw.columns)} â†’ {len(df_players_silver.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. CLEAN PICKS/BANS DATA\n",
    "print(\"\\n[5/6] Cleaning Picks/Bans data...\")\n",
    "\n",
    "df_picks_bans_clean = df_picks_bans_raw \\\n",
    "    .drop(\"Unnamed: 0\") \\\n",
    "    .filter(col(\"match_id\").isNotNull()) \\\n",
    "    .filter(col(\"hero_id\").isNotNull()) \\\n",
    "    .withColumn(\"is_pick\", col(\"is_pick\").cast(\"boolean\")) \\\n",
    "    .withColumn(\"hero_id\", col(\"hero_id\").cast(\"int\")) \\\n",
    "    .withColumn(\"team\", col(\"team\").cast(\"int\")) \\\n",
    "    .withColumn(\"pick_order\", col(\"order\").cast(\"int\")) \\\n",
    "    .drop(\"order\", \"ord\") \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "print(f\"   Picks/Bans cleaned: {df_picks_bans_clean.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcb8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. OUTLIER DETECTION\n",
    "print(\"\\n[5.5/6] Outlier Detection...\")\n",
    "\n",
    "stats = df_players_silver.select(\n",
    "    mean(\"kills\").alias(\"mean_kills\"),\n",
    "    stddev(\"kills\").alias(\"std_kills\"),\n",
    "    mean(\"deaths\").alias(\"mean_deaths\"),\n",
    "    stddev(\"deaths\").alias(\"std_deaths\"),\n",
    "    mean(\"gold_per_min\").alias(\"mean_gpm\"),\n",
    "    stddev(\"gold_per_min\").alias(\"std_gpm\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Player Statistics:\")\n",
    "print(f\"    Kills: mean={stats['mean_kills']:.2f}, std={stats['std_kills']:.2f}\")\n",
    "print(f\"    Deaths: mean={stats['mean_deaths']:.2f}, std={stats['std_deaths']:.2f}\")\n",
    "print(f\"    GPM: mean={stats['mean_gpm']:.2f}, std={stats['std_gpm']:.2f}\")\n",
    "\n",
    "df_players_silver = df_players_silver.withColumn(\n",
    "    \"is_outlier\",\n",
    "    when(\n",
    "        (col(\"kills\") > stats[\"mean_kills\"] + 3 * stats[\"std_kills\"]) |\n",
    "        (col(\"deaths\") > stats[\"mean_deaths\"] + 3 * stats[\"std_deaths\"]) |\n",
    "        (col(\"gold_per_min\") > stats[\"mean_gpm\"] + 3 * stats[\"std_gpm\"]),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "outlier_count = df_players_silver.filter(col(\"is_outlier\") == True).count()\n",
    "print(f\"   Outliers detected: {outlier_count:,} ({outlier_count/df_players_silver.count()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. WRITE TO SILVER LAYER AS DELTA\n",
    "print(\"\\n[6/6] Writing to Silver Layer...\")\n",
    "\n",
    "df_matches_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{SILVER_PATH}/cleaned_matches\")\n",
    "print(f\"   Matches written to: {SILVER_PATH}/cleaned_matches\")\n",
    "\n",
    "df_players_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{SILVER_PATH}/cleaned_players\")\n",
    "print(f\"   Players written to: {SILVER_PATH}/cleaned_players\")\n",
    "\n",
    "df_picks_bans_clean.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{SILVER_PATH}/cleaned_picks_bans\")\n",
    "print(f\"   Picks/Bans written to: {SILVER_PATH}/cleaned_picks_bans\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SILVER LAYER COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ckynwakf8rv",
   "source": "print(\"\\n[7/7] Exporting CSV Samples from Silver Layer...\")\n\nfrom pyspark.sql.functions import rand\nfrom datetime import datetime\n\nSAMPLE_SIZE_SILVER = 1000\nSAMPLES_PATH = f\"{SILVER_PATH}/samples\"\ntimestamp = datetime.now().strftime(\"%Y%m%d\")\n\nprint(f\"\\n  Exporting {SAMPLE_SIZE_SILVER} matches...\")\nmatches_sample = df_matches_silver.orderBy(rand(seed=42)).limit(SAMPLE_SIZE_SILVER)\nmatches_sample_file = f\"{SAMPLES_PATH}/silver_cleaned_matches_sample_{SAMPLE_SIZE_SILVER}rows_{timestamp}.csv\"\nmatches_sample.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(matches_sample_file)\nprint(f\"    Written to: {matches_sample_file}\")\n\nprint(f\"\\n  Exporting {SAMPLE_SIZE_SILVER} player records...\")\nplayers_sample = df_players_silver.orderBy(rand(seed=42)).limit(SAMPLE_SIZE_SILVER)\nplayers_sample_file = f\"{SAMPLES_PATH}/silver_cleaned_players_sample_{SAMPLE_SIZE_SILVER}rows_{timestamp}.csv\"\nplayers_sample.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(players_sample_file)\nprint(f\"    Written to: {players_sample_file}\")\n\nprint(f\"\\n  Exporting {SAMPLE_SIZE_SILVER} picks/bans records...\")\npicks_bans_sample = df_picks_bans_clean.orderBy(rand(seed=42)).limit(SAMPLE_SIZE_SILVER)\npicks_bans_sample_file = f\"{SAMPLES_PATH}/silver_cleaned_picks_bans_sample_{SAMPLE_SIZE_SILVER}rows_{timestamp}.csv\"\npicks_bans_sample.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(picks_bans_sample_file)\nprint(f\"    Written to: {picks_bans_sample_file}\")\n\nprint(\"\\n  Silver samples exported successfully!\")\nprint(f\"  Location: {SAMPLES_PATH}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b5982f",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n[Verification] Reading back from Silver...\")\n\ndf_verify_matches = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/cleaned_matches\")\ndf_verify_players = spark.read.format(\"delta\").load(f\"{SILVER_PATH}/cleaned_players\")\n\nprint(f\"  Silver Matches: {df_verify_matches.count():,} rows\")\nprint(f\"  Silver Players: {df_verify_players.count():,} rows\")\n\nprint(\"\\n  Sample Matches:\")\ndf_verify_matches.select(\"match_id\", \"duration_minutes\", \"radiant_win\", \"total_kills\", \"match_date\").show(5)\n\nprint(\"\\n  Sample Players:\")\ndf_verify_players.select(\"match_id\", \"account_id\", \"hero_id\", \"kills\", \"deaths\", \"assists\", \"kda\", \"win\").show(5)\n\nprint(\"\\n  CSV Samples created:\")\ntry:\n    sample_files = dbutils.fs.ls(f\"{SILVER_PATH}/samples\")\n    for f in sample_files:\n        if f.name.endswith('.csv/'):\n            print(f\"    {f.name}\")\nexcept Exception as e:\n    print(f\"    (No samples directory yet)\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}